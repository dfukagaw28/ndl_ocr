{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOEzPpiWWJfSykUv8hpmg/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakamura196/ndl_ocr/blob/main/CLIP%E3%82%92%E7%94%A8%E3%81%84%E3%81%9FText_to_Image%E3%81%A8Image_to_Image%E6%A4%9C%E7%B4%A2%E3%81%AE%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIPを用いたText-to-ImageとImage-to-Image検索のチュートリアル\n",
        "\n",
        "以下の記事を参考にしています。\n",
        "\n",
        "[Text-to-Image and Image-to-Image Search Using CLIP](https://www.pinecone.io/learn/clip-image-search/)"
      ],
      "metadata": {
        "id": "hv-rjq2JcoL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install transformers torch datasets"
      ],
      "metadata": {
        "id": "YRcwzCD8GdE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install gdcm\n",
        "!pip -qqq install pydicom\n",
        "!pip -qqq install faiss-gpu"
      ],
      "metadata": {
        "id": "FEhGm0dr3YjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import torch\n",
        "import skimage\n",
        "import requests\n",
        "# import pinecone\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import OrderedDict\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer"
      ],
      "metadata": {
        "id": "lhB56uxNGo3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataset\n",
        "image_data = load_dataset(\"conceptual_captions\", split=\"train\")"
      ],
      "metadata": {
        "id": "EaP2ITMH3jvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "URLをbase64でエンコードして、tmpフォルダに保存する"
      ],
      "metadata": {
        "id": "LnmrGdNQfTEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import hashlib\n",
        "\n",
        "def encode_and_get_path(url):\n",
        "  b = hashlib.md5(url.encode()).hexdigest()\n",
        "  return \"/tmp/\" + b + \".jpg\"\n",
        "\n",
        "def check_valid_URLs(image_URL):\n",
        "   try:\n",
        "     path = encode_and_get_path(image_URL)\n",
        "\n",
        "     if os.path.exists(path):\n",
        "       return True\n",
        "\n",
        "     response = requests.get(image_URL)\n",
        "     img = Image.open(BytesIO(response.content))\n",
        "\n",
        "     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "     img.save(path)\n",
        "\n",
        "     return True\n",
        "   except Exception as e:\n",
        "     print(e)\n",
        "     return False\n",
        "\n",
        "def get_image(image_URL):\n",
        "  path = encode_and_get_path(image_URL)\n",
        "\n",
        "  image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "-S1aJOg_3mNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 500\n",
        "!echo ダウンロードしたデータのうち、{size}件を使用する"
      ],
      "metadata": {
        "id": "F4leHVhqgfm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_data_df_all = image_data.to_pandas()\n",
        "image_data_df = image_data_df_all.head(size)"
      ],
      "metadata": {
        "id": "jZT0BAzJ4LwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Transform dataframe\n",
        "image_data_df[\"is_valid\"] = image_data_df[\"image_url\"].progress_apply(check_valid_URLs)\n",
        "# Get valid URLs\n",
        "image_data_df = image_data_df[image_data_df[\"is_valid\"]==True]\n",
        "# Get image from URL\n",
        "image_data_df[\"image\"] = image_data_df[\"image_url\"].progress_apply(get_image)"
      ],
      "metadata": {
        "id": "trOrJJwh3pCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_data_df"
      ],
      "metadata": {
        "id": "Jqj6yzq95v3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_info(model_ID, device):\n",
        "  # Save the model to device\n",
        "\tmodel = CLIPModel.from_pretrained(model_ID).to(device)\n",
        " \t# Get the processor\n",
        "\tprocessor = CLIPProcessor.from_pretrained(model_ID)\n",
        "  # Get the tokenizer\n",
        "\ttokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
        "       # Return model, processor & tokenizer\n",
        "\treturn model, processor, tokenizer\n",
        "# Set the device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Define the model ID\n",
        "model_ID = \"openai/clip-vit-base-patch32\"\n",
        "# Get model, processor & tokenizer\n",
        "model, processor, tokenizer = get_model_info(model_ID, device)"
      ],
      "metadata": {
        "id": "2iNPDY6Q3r08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9qP3QqiGMRB"
      },
      "outputs": [],
      "source": [
        "def get_single_text_embedding(text):\n",
        "  inputs = tokenizer(text, return_tensors = \"pt\").to(device)\n",
        "  text_embeddings = model.get_text_features(**inputs)\n",
        "  # convert the embeddings to numpy array\n",
        "  embedding_as_np = text_embeddings.cpu().detach().numpy()\n",
        "  return embedding_as_np\n",
        "\n",
        "def get_all_text_embeddings(df, text_col):\n",
        "  df[\"text_embeddings\"] = df[str(text_col)].apply(get_single_text_embedding)\n",
        "  return df\n",
        "\n",
        "# Apply the functions to the dataset\n",
        "image_data_df = get_all_text_embeddings(image_data_df, \"caption\")\n",
        "image_data_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_single_image_embedding(my_image):\n",
        "  image = processor(\n",
        "\t\ttext = None,\n",
        "\t\timages = my_image,\n",
        "\t\treturn_tensors=\"pt\"\n",
        "\t\t)[\"pixel_values\"].to(device)\n",
        "  embedding = model.get_image_features(image)\n",
        "  # convert the embeddings to numpy array\n",
        "  embedding_as_np = embedding.cpu().detach().numpy()\n",
        "  return embedding_as_np\n",
        "\n",
        "def get_all_images_embedding(df, img_column):\n",
        "\tdf[\"img_embeddings\"] = df[str(img_column)].apply(get_single_image_embedding)\n",
        "\treturn df\n",
        "\n",
        "image_data_df = get_all_images_embedding(image_data_df, \"image\")\n",
        "image_data_df"
      ],
      "metadata": {
        "id": "g4MEZpow6EAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def get_top_N_images(query, data, top_K=4, search_criterion=\"text\"):\n",
        "   # Text to image Search\n",
        "   if(search_criterion.lower() == \"text\"):\n",
        "     query_vect = get_single_text_embedding(query)\n",
        "   # Image to image Search\n",
        "   else:\n",
        "     query_vect = get_single_image_embedding(query)\n",
        "   # Relevant columns\n",
        "   revevant_cols = [\"caption\", \"image\", \"cos_sim\"]\n",
        "   # Run similarity Search\n",
        "   data[\"cos_sim\"] = data[\"img_embeddings\"].apply(lambda x: cosine_similarity(query_vect, x))# line 17\n",
        "   data[\"cos_sim\"] = data[\"cos_sim\"].apply(lambda x: x[0][0])\n",
        "   \"\"\"\n",
        "   Retrieve top_K (4 is default value) articles similar to the query\n",
        "   \"\"\"\n",
        "   most_similar_articles = data.sort_values(by='cos_sim',  ascending=False)[1:top_K+1] # line 24\n",
        "   return most_similar_articles[revevant_cols].reset_index()"
      ],
      "metadata": {
        "id": "_xOYhpdN7CSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images_by_side(top_images):\n",
        " index_values = list(top_images.index.values)\n",
        " list_images = [top_images.iloc[idx].image for idx in index_values]\n",
        " list_captions = [top_images.iloc[idx].caption for idx in index_values]\n",
        " similarity_score = [top_images.iloc[idx].cos_sim for idx in index_values]\n",
        " n_row = n_col = 2\n",
        " _, axs = plt.subplots(n_row, n_col, figsize=(12, 12))\n",
        " axs = axs.flatten()\n",
        " for img, ax, caption, sim_score in zip(list_images, axs, list_captions, similarity_score):\n",
        "     ax.imshow(img)\n",
        "     sim_score = 100*float(\"{:.2f}\".format(sim_score))\n",
        "     ax.title.set_text(f\"Caption: {caption}\\nSimilarity: {sim_score}%\")\n",
        " plt.show()"
      ],
      "metadata": {
        "id": "7D7hCmuJ7L6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query_caption = image_data_df.iloc[10].caption\n",
        "query_caption = \"computer\"\n",
        "# Print the original query text\n",
        "print(\"Query: {}\".format(query_caption))\n",
        "# Run the similarity search\n",
        "top_images = get_top_N_images(query_caption, image_data_df)\n",
        "# Plot the recommended images\n",
        "plot_images_by_side(top_images)"
      ],
      "metadata": {
        "id": "hmvjlR8w7Ryb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the query image and show it\n",
        "query_image = image_data_df.iloc[30].image\n",
        "query_image"
      ],
      "metadata": {
        "id": "kQmMhkRU7ZJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the similarity search and plot the result\n",
        "top_images = get_top_N_images(query_image, image_data_df, search_criterion=\"image\")\n",
        "# Plot the result\n",
        "plot_images_by_side(top_images)"
      ],
      "metadata": {
        "id": "CYg2yTvI7aSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}