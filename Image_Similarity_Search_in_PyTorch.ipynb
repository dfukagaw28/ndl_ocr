{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakamura196/ndl_ocr/blob/main/Image_Similarity_Search_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9-HrzSxpLpF"
      },
      "source": [
        "#  Image Similarity Search in PyTorch\n",
        "\n",
        "以下を参考にしています。\n",
        "\n",
        "https://medium.com/pytorch/image-similarity-search-in-pytorch-1a744cf3469"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv7Z0_HSMSg7",
        "outputId": "19e4f6bc-4458-41f6-cd37-8ff5a47cb461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 17 03:47:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    39W / 300W |   9263MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HOME_DIR = \"/content\"\n",
        "IMG_DIR = \"/content/dataset\"\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256"
      ],
      "metadata": {
        "id": "qkmZSMB255IW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dataset`フォルダがない場合には、データをダウンロード"
      ],
      "metadata": {
        "id": "zOzPbzNx4wqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(IMG_DIR):\n",
        "  !wget --no-check-certificate \\\n",
        "      https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "      -O cats_and_dogs_filtered.zip\n",
        "  !unzip cats_and_dogs_filtered.zip\n",
        "  !mkdir dataset\n",
        "\n",
        "  import glob\n",
        "  from tqdm import tqdm\n",
        "  files = glob.glob(\"/content/cats_and_dogs_filtered/*/*/*.jpg\")\n",
        "  for file in tqdm(files):\n",
        "    filename = file.split(\"/\")[-1]\n",
        "    !mv $file $IMG_DIR/$filename\n",
        "\n",
        "  !rm -rf cats_and_dogs_filtered"
      ],
      "metadata": {
        "id": "vRsVq-2BuMPp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2t0EdUSRMiv0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "1rxToCLuMle2"
      },
      "outputs": [],
      "source": [
        "class FolderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Creates a PyTorch dataset from folder, returning two tensor images.\n",
        "    Args: \n",
        "    main_dir : directory where images are stored.\n",
        "    transform (optional) : torchvision transforms to be applied while making dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, main_dir, transform=None):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        self.all_imgs = os.listdir(main_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "\n",
        "        image = image.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            tensor_image = self.transform(image)\n",
        "\n",
        "        return tensor_image, tensor_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "9LnRgNXRMm71"
      },
      "outputs": [],
      "source": [
        "class ConvEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # self.img_size = img_size\n",
        "        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        '''\n",
        "        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n",
        "        self.relu5 = nn.ReLU(inplace=True)\n",
        "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
        "        '''\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Downscale the image with conv maxpool etc.\n",
        "        # print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "\n",
        "        '''\n",
        "        x = self.conv5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.maxpool5(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "        \n",
        "        '''\n",
        "        return x\n",
        "\n",
        "class ConvDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Convolutional Decoder Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n",
        "        # self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
        "        # self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n",
        "        # self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.deconv4 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n",
        "        # self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.relu1(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.deconv2(x)\n",
        "        x = self.relu2(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.deconv3(x)\n",
        "        x = self.relu3(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.deconv4(x)\n",
        "        x = self.relu4(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "O2FOxTRJMpdh"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "TRAIN_RATIO = 0.75\n",
        "VAL_RATIO = 1 - TRAIN_RATIO\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 20\n",
        "\n",
        "TRAIN_BATCH_SIZE = 256  # Let's see, I don't have GPU, Google Colab is best hope\n",
        "TEST_BATCH_SIZE = 256  # Let's see, I don't have GPU, Google Colab is best hope\n",
        "FULL_BATCH_SIZE = 256\n",
        "\n",
        "AUTOENCODER_MODEL_PATH = f\"{HOME_DIR}/baseline_autoencoder.pt\"\n",
        "ENCODER_MODEL_PATH = f\"{HOME_DIR}/baseline_encoder.pt\"\n",
        "DECODER_MODEL_PATH = f\"{HOME_DIR}/baseline_decoder.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "t7ziVRdWQM7s"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "I can write this if we need custom training loop etc.\n",
        "I usually use this in PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "__all__ = [\"train_step\", \"val_step\", \"create_embedding\"]\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):\n",
        "    # device = \"cuda\"\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # print(device)\n",
        "\n",
        "    for batch_idx, (train_img, target_img) in enumerate(train_loader):\n",
        "        train_img = train_img.to(device)\n",
        "        target_img = target_img.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        enc_output = encoder(train_img)\n",
        "        dec_output = decoder(enc_output)\n",
        "\n",
        "        loss = loss_fn(dec_output, target_img)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def val_step(encoder, decoder, val_loader, loss_fn, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (train_img, target_img) in enumerate(val_loader):\n",
        "            train_img = train_img.to(device)\n",
        "            target_img = target_img.to(device)\n",
        "\n",
        "            enc_output = encoder(train_img)\n",
        "            dec_output = decoder(enc_output)\n",
        "\n",
        "            loss = loss_fn(dec_output, target_img)\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhe3znZhQW5Y",
        "outputId": "400e5b6e-275c-4086-aa2d-1595b140d70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Creating Dataset ------------\n",
            "------------ Dataset Created ------------\n",
            "------------ Creating DataLoader ------------\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# print(\"Setting Seed for the run, seed = {}\".format(config.SEED))\n",
        "\n",
        "# seed_everything(config.SEED)\n",
        "\n",
        "transforms = T.Compose([T.ToTensor()])\n",
        "print(\"------------ Creating Dataset ------------\")\n",
        "full_dataset = FolderDataset(IMG_DIR, transforms)\n",
        "\n",
        "train_size = int(TRAIN_RATIO * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "print(\"------------ Dataset Created ------------\")\n",
        "print(\"------------ Creating DataLoader ------------\")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=TEST_BATCH_SIZE\n",
        ")\n",
        "\n",
        "full_loader = torch.utils.data.DataLoader(\n",
        "    full_dataset, batch_size=FULL_BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = train_dataset.dataset.all_imgs"
      ],
      "metadata": {
        "id": "xsvXAqn9vPot"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9IEoFxMb84Zz"
      },
      "outputs": [],
      "source": [
        "lines = []\n",
        "for img in imgs:\n",
        "  lines.append(img + \"\\n\")\n",
        "with open(f\"{HOME_DIR}/output.txt\", 'w', encoding='utf-8', newline='\\n') as f:\n",
        "    f.writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpv3TvtEQiGB",
        "outputId": "442c7660-0740-4968-8be8-5cba3aea9f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Dataloader Cretead ------------\n",
            "GPU Availaible moving models to GPU\n",
            "------------ Training started ------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs = 0, Training Loss : 0.22409912943840027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:17<05:32, 17.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss decreased, saving new best model\n",
            "Epochs = 0, Validation Loss : 0.22495345771312714\n",
            "Epochs = 1, Training Loss : 0.2089099884033203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [00:34<05:14, 17.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss decreased, saving new best model\n",
            "Epochs = 1, Validation Loss : 0.19292905926704407\n",
            "Epochs = 2, Training Loss : 0.1672382354736328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [00:52<04:54, 17.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss decreased, saving new best model\n",
            "Epochs = 2, Validation Loss : 0.16914509236812592\n"
          ]
        }
      ],
      "source": [
        "print(\"------------ Dataloader Cretead ------------\")\n",
        "\n",
        "# print(train_loader)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "encoder = ConvEncoder()\n",
        "decoder = ConvDecoder()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Availaible moving models to GPU\")\n",
        "else:\n",
        "    print(\"Moving models to CPU\")\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "# print(device)\n",
        "\n",
        "autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.AdamW(autoencoder_params, lr=LEARNING_RATE)\n",
        "\n",
        "# early_stopper = utils.EarlyStopping(patience=5, verbose=True, path=)\n",
        "max_loss = 9999\n",
        "\n",
        "print(\"------------ Training started ------------\")\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss = train_step(\n",
        "        encoder, decoder, train_loader, loss_fn, optimizer, device=device\n",
        "    )\n",
        "    print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
        "    val_loss = val_step(\n",
        "        encoder, decoder, val_loader, loss_fn, device=device\n",
        "    )\n",
        "\n",
        "    # Simple Best Model saving\n",
        "    if val_loss < max_loss:\n",
        "        print(\"Validation Loss decreased, saving new best model\")\n",
        "        torch.save(encoder.state_dict(), ENCODER_MODEL_PATH)\n",
        "        torch.save(decoder.state_dict(), DECODER_MODEL_PATH)\n",
        "\n",
        "    print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
        "\n",
        "print(\"Training Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50gKDfz2Qoqr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PrH0FMGQqzq"
      },
      "outputs": [],
      "source": [
        "def create_embedding(encoder, full_loader, embedding_dim, device):\n",
        "    encoder.eval()\n",
        "    embedding = torch.randn(embedding_dim)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (train_img, target_img) in tqdm(enumerate(full_loader)):\n",
        "            train_img = train_img.to(device)\n",
        "            enc_output = encoder(train_img).cpu()\n",
        "            embedding = torch.cat((embedding, enc_output), 0)\n",
        "    \n",
        "    return embedding\n",
        "\n",
        "# Save the feature representations.\n",
        "embedding_dim = (1, 128, 16, 16) # This we know from our encoder\n",
        "\n",
        "# We need feature representations for complete dataset not just train and validation.\n",
        "# Hence we use full loader here.\n",
        "embedding = create_embedding(encoder, full_loader, embedding_dim, device)\n",
        "\n",
        "# Convert embedding to numpy and save them\n",
        "numpy_embedding = embedding.cpu().detach().numpy()\n",
        "num_images = numpy_embedding.shape[0]\n",
        "\n",
        "# Save the embeddings for complete dataset, not just train\n",
        "flattened_embedding = numpy_embedding.reshape((num_images, -1))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.save(f\"{HOME_DIR}/data_embedding_f.npy\", flattened_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKyH9nE-TY95"
      },
      "source": [
        "# 推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2v4tgd8TNPS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N6TVlt4TbdB"
      },
      "outputs": [],
      "source": [
        "size = IMG_WIDTH\n",
        "\n",
        "def load_image_tensor(image_path, device):\n",
        "    img = Image.open(image_path)\n",
        "    img = img.resize((size, size))\n",
        "    image_tensor = T.ToTensor()(img)\n",
        "    image_tensor = image_tensor.unsqueeze(0)\n",
        "    return image_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Huzq_NTc3L"
      },
      "outputs": [],
      "source": [
        "def compute_similar_images(image_path, num_images, embedding, device):\n",
        "    image_tensor = load_image_tensor(image_path, device)\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_embedding = encoder(image_tensor).cpu().detach().numpy()\n",
        "\n",
        "    flattened_embedding = image_embedding.reshape((image_embedding.shape[0], -1))\n",
        "\n",
        "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
        "    knn.fit(embedding)\n",
        "\n",
        "    _, indices = knn.kneighbors(flattened_embedding)\n",
        "    indices_list = indices.tolist()\n",
        "    return indices_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN2dO78nTedV"
      },
      "outputs": [],
      "source": [
        "imgs = train_dataset.dataset.all_imgs\n",
        "\n",
        "def plot_similar_images(indices_list):\n",
        "    indices = indices_list[0]\n",
        "\n",
        "    for index in indices:\n",
        "        img_name = train_dataset.dataset.all_imgs[index - 1]\n",
        "        img_path = os.path.join(f\"{IMG_DIR}/{img_name}\")\n",
        "        print(img_path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        plt.imshow(img)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4qSDF2iTgTE"
      },
      "outputs": [],
      "source": [
        "NUM_IMAGES = 10\n",
        "ENCODER_MODEL_PATH = f\"{HOME_DIR}/baseline_encoder.pt\"\n",
        "EMBEDDING_PATH = f\"{HOME_DIR}/data_embedding_f.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_YZ_s6tT9KR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "encoder = ConvEncoder() # torch_model.\n",
        "\n",
        "# Load the state dict of encoder\n",
        "encoder.load_state_dict(torch.load(ENCODER_MODEL_PATH, map_location=device))\n",
        "encoder.eval()\n",
        "encoder.to(device)\n",
        "\n",
        "# Loads the embedding\n",
        "embedding = np.load(EMBEDDING_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "猫の例"
      ],
      "metadata": {
        "id": "MvBDBZ614qTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_IMAGE_PATH = \"/content/dataset/cat.0.jpg\"\n",
        "indices_list = compute_similar_images(TEST_IMAGE_PATH, NUM_IMAGES, embedding, device)\n",
        "plot_similar_images(indices_list)"
      ],
      "metadata": {
        "id": "FeN-ztpA4fs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "犬の例"
      ],
      "metadata": {
        "id": "PGdq2a9d4nGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_IMAGE_PATH = \"/content/dataset/dog.0.jpg\"\n",
        "indices_list = compute_similar_images(TEST_IMAGE_PATH, NUM_IMAGES, embedding, device)\n",
        "plot_similar_images(indices_list)"
      ],
      "metadata": {
        "id": "EFBXusp24hqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Image Similarity Search in PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVdkGChvnsg5uRVtBNCDlw",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}